{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Model for Crypto Price Prediction\n",
    "\n",
    "**Target**: Predicting whether the crypto price will be higher in 5 days than it is today  \n",
    "**Method**: Transformer - Self-Attention Neural Network with TensorFlow/Keras\n",
    "\n",
    "## What is a Transformer?\n",
    "- **Self-Attention Mechanism**: Learns which past timesteps are most important for prediction\n",
    "- **Parallel Processing**: Unlike LSTM/GRU, processes all timesteps simultaneously (faster!)\n",
    "- **Long-range Dependencies**: Can directly connect Day 1 to Day 20 (no vanishing gradient)\n",
    "- **Multi-Head Attention**: Multiple attention patterns learn different temporal relationships\n",
    "- **State-of-the-art**: Currently dominates NLP, increasingly used for time series\n",
    "- **Interpretable**: Can visualize which past days the model focuses on\n",
    "\n",
    "## Transformer vs LSTM/GRU:\n",
    "| Feature | LSTM/GRU | Transformer |\n",
    "|---------|----------|-------------|\n",
    "| **Processing** | Sequential (slow) | Parallel (fast) |\n",
    "| **Memory** | Hidden state (limited) | Self-attention (unlimited) |\n",
    "| **Long-term** | Vanishing gradient issue | Direct attention |\n",
    "| **Parameters** | ~35K | ~100K+ |\n",
    "| **Interpretability** | Black box | Attention weights visible |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.19.0\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "SEED = 101\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Keras layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization,\n",
    "    MultiHeadAttention, GlobalAveragePooling1D\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "print(f\"TensorFlow {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200,973 rows, 78 columns\n",
      "Cryptocurrencies: 142\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df_transformer = pd.read_csv(\"/Users/ina/Documents/spicedAcademy/time_series_project/data/final_data/20251115_dataset_crp.csv\")\n",
    "\n",
    "print(f\"Loaded {df_transformer.shape[0]:,} rows, {df_transformer.shape[1]} columns\")\n",
    "print(f\"Cryptocurrencies: {df_transformer['ticker'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 142034 samples\n",
      "Validation set: 9543 samples\n",
      "Test set: 39328 samples\n",
      "Train up to date: 2024-06-30 00:00:00+00:00\n",
      "Validation from date: 2024-07-01 00:00:00+00:00 to 2024-09-30 00:00:00+00:00\n",
      "Test from date: 2025-01-01 00:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "# Train-Validation-Test Split (70%-15%-15% temporal split)\n",
    "split_date_train = \"2024-07-01\"  # Training until this date\n",
    "split_date_val = \"2024-10-01\"    # Validation from here\n",
    "split_date_test = \"2025-01-01\"   # Testing from here\n",
    "\n",
    "df_train = df_transformer[df_transformer['date'] < split_date_train].copy()\n",
    "df_val = df_transformer[(df_transformer['date'] >= split_date_train) & (df_transformer['date'] < split_date_val)].copy()\n",
    "df_test = df_transformer[df_transformer['date'] >= split_date_test].copy()\n",
    "\n",
    "print(f\"\\nTraining set: {len(df_train)} samples\")\n",
    "print(f\"Validation set: {len(df_val)} samples\")\n",
    "print(f\"Test set: {len(df_test)} samples\")\n",
    "print(f\"Train up to date: {df_train['date'].max()}\")\n",
    "print(f\"Validation from date: {df_val['date'].min()} to {df_val['date'].max()}\")\n",
    "print(f\"Test from date: {df_test['date'].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features: (142034, 68), Target: (142034,)\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and targets\n",
    "\n",
    "# Target columns to exclude\n",
    "target_columns = [\n",
    "    'future_5_close_higher_than_today',\n",
    "    'future_10_close_higher_than_today',\n",
    "    'future_5_close_lower_than_today',\n",
    "    'future_10_close_lower_than_today',\n",
    "    'higher_close_today_vs_future_5_close',\n",
    "    'higher_close_today_vs_future_10_close',\n",
    "    'lower_close_today_vs_future_5_close',\n",
    "    'lower_close_today_vs_future_10_close'\n",
    "]\n",
    "\n",
    "# Get feature columns\n",
    "feature_columns = []\n",
    "for col in df_train.columns:\n",
    "    if col not in target_columns:\n",
    "        feature_columns.append(col)\n",
    "\n",
    "# Split features and target\n",
    "X_train = df_train[feature_columns].copy()\n",
    "y_train = df_train[\"future_5_close_higher_than_today\"].values\n",
    "\n",
    "X_val = df_val[feature_columns].copy()\n",
    "y_val = df_val[\"future_5_close_higher_than_today\"].values\n",
    "\n",
    "X_test = df_test[feature_columns].copy()\n",
    "y_test = df_test[\"future_5_close_higher_than_today\"].values\n",
    "\n",
    "# Scale data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Columns to scale (exclude ticker and date)\n",
    "columns_to_scale = []\n",
    "for col in X_train.columns:\n",
    "    if col not in ['ticker', 'date']:\n",
    "        columns_to_scale.append(col)\n",
    "\n",
    "# Fit and transform\n",
    "X_train_scaled_vals = scaler_X.fit_transform(X_train[columns_to_scale])\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
    "\n",
    "X_val_scaled_vals = scaler_X.transform(X_val[columns_to_scale])\n",
    "y_val_scaled = scaler_y.transform(y_val.reshape(-1, 1)).ravel()\n",
    "\n",
    "X_test_scaled_vals = scaler_X.transform(X_test[columns_to_scale])\n",
    "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
    "\n",
    "print(f\"✅ Features: {X_train_scaled_vals.shape}, Target: {y_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after creating squences: (139766, 20, 68)\n"
     ]
    }
   ],
   "source": [
    "# Create sequences for transformer\n",
    "look_back = 20\n",
    "\n",
    "# Training sequences\n",
    "X_train_sequences = []\n",
    "y_train_sequences = []\n",
    "train_row_indices = []\n",
    "\n",
    "for crypto in X_train['ticker'].unique():\n",
    "    # Get data for this crypto\n",
    "    is_this_crypto = X_train['ticker'] == crypto\n",
    "    crypto_features = X_train_scaled_vals[is_this_crypto]\n",
    "    crypto_targets = y_train_scaled[is_this_crypto]\n",
    "    crypto_dates = X_train.loc[is_this_crypto, 'date'].values\n",
    "    crypto_rows = X_train[is_this_crypto].index.values\n",
    "    \n",
    "    # Sort by date\n",
    "    date_order = np.argsort(crypto_dates)\n",
    "    crypto_features = crypto_features[date_order]\n",
    "    crypto_targets = crypto_targets[date_order]\n",
    "    crypto_rows = crypto_rows[date_order]\n",
    "    \n",
    "    # Create sliding windows\n",
    "    num_sequences = len(crypto_features) - look_back\n",
    "    for i in range(num_sequences):\n",
    "        sequence = crypto_features[i : i + look_back]\n",
    "        target = crypto_targets[i + look_back]\n",
    "        row = crypto_rows[i + look_back]\n",
    "        \n",
    "        X_train_sequences.append(sequence)\n",
    "        y_train_sequences.append(target)\n",
    "        train_row_indices.append(row)\n",
    "\n",
    "# Convert to arrays\n",
    "X_train_seq = np.array(X_train_sequences)\n",
    "y_train_seq = np.array(y_train_sequences)\n",
    "train_indices = np.array(train_row_indices)\n",
    "\n",
    "# Validation sequences\n",
    "X_val_sequences = []\n",
    "y_val_sequences = []\n",
    "val_row_indices = []\n",
    "\n",
    "for crypto in X_val['ticker'].unique():\n",
    "    is_this_crypto = X_val['ticker'] == crypto\n",
    "    crypto_features = X_val_scaled_vals[is_this_crypto]\n",
    "    crypto_targets = y_val_scaled[is_this_crypto]\n",
    "    crypto_dates = X_val.loc[is_this_crypto, 'date'].values\n",
    "    crypto_rows = X_val[is_this_crypto].index.values\n",
    "    \n",
    "    date_order = np.argsort(crypto_dates)\n",
    "    crypto_features = crypto_features[date_order]\n",
    "    crypto_targets = crypto_targets[date_order]\n",
    "    crypto_rows = crypto_rows[date_order]\n",
    "    \n",
    "    num_sequences = len(crypto_features) - look_back\n",
    "    for i in range(num_sequences):\n",
    "        sequence = crypto_features[i : i + look_back]\n",
    "        target = crypto_targets[i + look_back]\n",
    "        row = crypto_rows[i + look_back]\n",
    "        \n",
    "        X_val_sequences.append(sequence)\n",
    "        y_val_sequences.append(target)\n",
    "        val_row_indices.append(row)\n",
    "\n",
    "X_val_seq = np.array(X_val_sequences)\n",
    "y_val_seq = np.array(y_val_sequences)\n",
    "val_indices = np.array(val_row_indices)\n",
    "\n",
    "# Test sequences\n",
    "X_test_sequences = []\n",
    "y_test_sequences = []\n",
    "test_row_indices = []\n",
    "\n",
    "for crypto in X_test['ticker'].unique():\n",
    "    is_this_crypto = X_test['ticker'] == crypto\n",
    "    crypto_features = X_test_scaled_vals[is_this_crypto]\n",
    "    crypto_targets = y_test_scaled[is_this_crypto]\n",
    "    crypto_dates = X_test.loc[is_this_crypto, 'date'].values\n",
    "    crypto_rows = X_test[is_this_crypto].index.values\n",
    "    \n",
    "    date_order = np.argsort(crypto_dates)\n",
    "    crypto_features = crypto_features[date_order]\n",
    "    crypto_targets = crypto_targets[date_order]\n",
    "    crypto_rows = crypto_rows[date_order]\n",
    "    \n",
    "    num_sequences = len(crypto_features) - look_back\n",
    "    for i in range(num_sequences):\n",
    "        sequence = crypto_features[i : i + look_back]\n",
    "        target = crypto_targets[i + look_back]\n",
    "        row = crypto_rows[i + look_back]\n",
    "        \n",
    "        X_test_sequences.append(sequence)\n",
    "        y_test_sequences.append(target)\n",
    "        test_row_indices.append(row)\n",
    "\n",
    "X_test_seq = np.array(X_test_sequences)\n",
    "y_test_seq = np.array(y_test_sequences)\n",
    "test_indices = np.array(test_row_indices)\n",
    "\n",
    "# Get number of features\n",
    "n_features = X_train_seq.shape[2]\n",
    "\n",
    "print(f\"Shape after creating squences: {X_train_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encoding shape: (20, 68)\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding: transformers process all timesteps in parallel\n",
    "# We need to give each day a unique position identifier\n",
    "\n",
    "# How many days and features?\n",
    "num_days = 20\n",
    "num_features = 68\n",
    "\n",
    "# Create position numbers [0, 1, 2, ..., 19]\n",
    "position_numbers = np.arange(num_days)\n",
    "position_numbers = position_numbers[:, np.newaxis]  # Reshape to column\n",
    "\n",
    "# Create frequency values (formula from research paper)\n",
    "even_numbers = np.arange(0, num_features, 2)\n",
    "frequencies = np.exp(even_numbers * -(np.log(10000.0) / num_features))\n",
    "\n",
    "# Create empty table: 20 rows × 68 columns\n",
    "position_encoding_table = np.zeros((num_days, num_features))\n",
    "\n",
    "# Fill with sine/cosine patterns\n",
    "position_encoding_table[:, 0::2] = np.sin(position_numbers * frequencies)  # Even columns\n",
    "position_encoding_table[:, 1::2] = np.cos(position_numbers * frequencies)  # Odd columns\n",
    "\n",
    "# Convert to TensorFlow format\n",
    "positional_encoding = tf.constant(position_encoding_table, dtype=tf.float32)\n",
    "\n",
    "print(f\"Positional encoding shape: {positional_encoding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer settings\n",
    "head_size = 128\n",
    "num_heads = 4\n",
    "dropout_rate = 0.2\n",
    "ff_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">140,868</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│                     │                   │            │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,832</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,772</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">140,868</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MultiHeadAttentio…</span> │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,832</span> │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,772</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ layer_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,416</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add (\u001b[38;5;33mAdd\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │    \u001b[38;5;34m140,868\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│                     │                   │            │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │        \u001b[38;5;34m136\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m8,832\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │      \u001b[38;5;34m8,772\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_2 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │        \u001b[38;5;34m136\u001b[0m │ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ multi_head_attenti… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │    \u001b[38;5;34m140,868\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mMultiHeadAttentio…\u001b[0m │                   │            │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ multi_head_atten… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_3 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │        \u001b[38;5;34m136\u001b[0m │ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │      \u001b[38;5;34m8,832\u001b[0m │ layer_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │      \u001b[38;5;34m8,772\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│                     │                   │            │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ layer_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m68\u001b[0m)    │        \u001b[38;5;34m136\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mLayerNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ layer_normalizat… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,416\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">321,969</span> (1.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m321,969\u001b[0m (1.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">321,969</span> (1.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m321,969\u001b[0m (1.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build transformer model\n",
    "learning_rate = 0.0001\n",
    "mlp_dropout = 0.3\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(look_back, n_features))\n",
    "\n",
    "# Add positional encoding\n",
    "x = inputs + positional_encoding\n",
    "\n",
    "# Transformer Block 1\n",
    "# Multi-head attention\n",
    "attention_1 = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout_rate)(x, x)\n",
    "attention_1 = Dropout(dropout_rate)(attention_1)\n",
    "\n",
    "# Residual connection\n",
    "attention_1 = x + attention_1\n",
    "attention_1 = LayerNormalization(epsilon=1e-6)(attention_1)\n",
    "\n",
    "# Feed-forward network\n",
    "ffn_1 = Dense(ff_dim, activation=\"relu\")(attention_1)\n",
    "ffn_1 = Dropout(dropout_rate)(ffn_1)\n",
    "ffn_1 = Dense(n_features)(ffn_1)\n",
    "ffn_1 = Dropout(dropout_rate)(ffn_1)\n",
    "\n",
    "# Residual connection\n",
    "ffn_1 = attention_1 + ffn_1\n",
    "block_1_output = LayerNormalization(epsilon=1e-6)(ffn_1)\n",
    "\n",
    "# Transformer Block 2\n",
    "# Multi-head attention\n",
    "attention_2 = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout_rate)(block_1_output, block_1_output)\n",
    "attention_2 = Dropout(dropout_rate)(attention_2)\n",
    "\n",
    "# Residual connection\n",
    "attention_2 = block_1_output + attention_2\n",
    "attention_2 = LayerNormalization(epsilon=1e-6)(attention_2)\n",
    "\n",
    "# Feed-forward network\n",
    "ffn_2 = Dense(ff_dim, activation=\"relu\")(attention_2)\n",
    "ffn_2 = Dropout(dropout_rate)(ffn_2)\n",
    "ffn_2 = Dense(n_features)(ffn_2)\n",
    "ffn_2 = Dropout(dropout_rate)(ffn_2)\n",
    "\n",
    "# Residual connection\n",
    "ffn_2 = attention_2 + ffn_2\n",
    "block_2_output = LayerNormalization(epsilon=1e-6)(ffn_2)\n",
    "\n",
    "# Pool across time dimension\n",
    "pooled = GlobalAveragePooling1D()(block_2_output)\n",
    "\n",
    "# Final prediction layers\n",
    "prediction = Dense(64, activation=\"relu\")(pooled)\n",
    "prediction = Dropout(mlp_dropout)(prediction)\n",
    "outputs = Dense(1, activation=\"linear\")(prediction)\n",
    "\n",
    "# Create model\n",
    "model_transformer = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model_transformer.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"mae\",\n",
    "    metrics=[\"mae\", \"mse\"]\n",
    ")\n",
    "model_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1092/1092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 104ms/step - loss: 0.7423 - mae: 0.7423 - mse: 1.0277 - val_loss: 0.7147 - val_mae: 0.7147 - val_mse: 0.8853 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m1092/1092\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 0.6981 - mae: 0.6981 - mse: 0.9250"
     ]
    }
   ],
   "source": [
    "# Train transformer model\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "patience = 15\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=patience,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate if stuck\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model_transformer.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics from the last epoch\n",
    "mae_train = history.history['mae'][-1]\n",
    "mse_train = history.history['mse'][-1]\n",
    "print(\"Train Set Performance:\")\n",
    "print(f\"   MAE:  {mae_train:.4f}\")\n",
    "print(f\"   MSE:  {mse_train:.4f}\")\n",
    "\n",
    "val_mae = history.history['val_mae'][-1]\n",
    "val_mse = history.history['val_mse'][-1]\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"   MAE:  {val_mae:.4f}\")\n",
    "print(f\"   MSE:  {val_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history - Transformer\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "\n",
    "# MAE\n",
    "axes[0].plot(history.history['mae'], label='training mae', linewidth=2)\n",
    "axes[0].plot(history.history['val_mae'], label='validation mae', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('MAE')\n",
    "axes[0].set_title('Transformer: Mean Absolute Error')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MSE\n",
    "axes[1].plot(history.history['mse'], label='training mse', linewidth=2)\n",
    "axes[1].plot(history.history['val_mse'], label='validation mse', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MSE')\n",
    "axes[1].set_title('Transformer: Mean Squared Error')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "y_pred_scaled = model_transformer.predict(X_test_seq)\n",
    "\n",
    "# Metrics on scaled data\n",
    "mae = mean_absolute_error(y_test_seq, y_pred_scaled)\n",
    "mse = mean_squared_error(y_test_seq, y_pred_scaled)\n",
    "\n",
    "print(\"Test Set Performance (Scaled Data):\")\n",
    "print(f\"   MAE:  {mae:.4f}\")\n",
    "print(f\"   MSE:  {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INVERSE TRANSFORM - Convert predictions back to original scale\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_pred_original = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "# Get actual original values using the tracked indices\n",
    "y_test_original = df_test.loc[test_indices, 'future_5_close_higher_than_today'].values\n",
    "\n",
    "# Calculate metrics on ORIGINAL scale\n",
    "mae = mean_absolute_error(y_test_original, y_pred_original)\n",
    "mse = mean_squared_error(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"Test Set Performance (Original Scale):\")\n",
    "print(f\"   MAE:  {mae:.4f} percentage points\")\n",
    "print(f\"   MSE:  {mse:.4f}\")\n",
    "print(f\"\\nNote: {len(y_pred_original)} predictions aligned with original data using indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Predicted vs Actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_seq, y_pred_scaled, alpha=0.5, s=10)\n",
    "plt.plot([y_test_seq.min(), y_test_seq.max()], \n",
    "         [y_test_seq.min(), y_test_seq.max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Values (Scaled)')\n",
    "plt.ylabel('Predicted Values (Scaled)')\n",
    "plt.title('Transformer: Predicted vs Actual (Test Set)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Residuals Analysis - Transformer\n",
    "residuals = y_test_original - y_pred_original\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals histogram\n",
    "axes[0].hist(residuals, bins=50, edgecolor='black')\n",
    "axes[0].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Transformer: Distribution of Residuals')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[1].scatter(y_pred_original, residuals, alpha=0.5, s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].set_title('Transformer: Residuals vs Predicted')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Residual Stats:\")\n",
    "print(f\"   Mean: {residuals.mean():.4f}\")\n",
    "print(f\"   Std:  {residuals.std():.4f}\")\n",
    "print(f\"   Min:  {residuals.min():.4f}\")\n",
    "print(f\"   Max:  {residuals.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.save('/Users/ina/Documents/spicedAcademy/time_series_project//models/model_transformer.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
