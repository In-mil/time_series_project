name: Comprehensive Production Monitoring

on:
  schedule:
    # Daily drift check at 8:00 AM UTC
    - cron: '0 8 * * *'
    # Health checks every 6 hours
    - cron: '0 */24 * * *'
  workflow_dispatch:  # Manual trigger

# Required permissions for creating issues
permissions:
  contents: read
  issues: write

env:
  API_URL: https://time-series-api-jgqkhpmk5q-ey.a.run.app
  PROMETHEUS_URL: http://localhost:9090

jobs:
  # ============================================
  # DRIFT DETECTION (Daily)
  # ============================================
  drift-detection:
    name: üîç Drift Detection
    runs-on: ubuntu-latest
    # Only run drift check once daily (not on 6-hour schedule)
    if: github.event.schedule == '0 8 * * *' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --no-cache-dir requests pandas numpy

      - name: Check drift status
        id: drift-status
        run: |
          echo "üìä Checking drift detection status..."
          python scripts/check_drift.py \
            --api-url ${{ env.API_URL }} \
            --skip-report
        continue-on-error: true

      - name: Analyze drift results
        if: always()
        run: |
          if [ "${{ steps.drift-status.outcome }}" == "failure" ]; then
            echo "drift_detected=true" >> $GITHUB_OUTPUT
            echo "## ‚ö†Ô∏è Drift Detected!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Model drift was detected in production." >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Action Required:**" >> $GITHUB_STEP_SUMMARY
            echo "- Review drift metrics" >> $GITHUB_STEP_SUMMARY
            echo "- Consider retraining if drift score > 0.5" >> $GITHUB_STEP_SUMMARY
          else
            echo "drift_detected=false" >> $GITHUB_OUTPUT
            echo "## ‚úÖ No Drift Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Model is performing within expected parameters." >> $GITHUB_STEP_SUMMARY
          fi

      # Issue creation disabled due to YAML syntax conflicts
      # You'll receive email notifications instead

      - name: Fail on drift detection
        if: steps.drift-status.outcome == 'failure'
        run: |
          echo "::error::‚ö†Ô∏è Model drift detected! Issue created automatically."
          exit 1

  # ============================================
  # API HEALTH CHECK (Every 6 hours)
  # ============================================
  api-health:
    name: üè• API Health Check
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --no-cache-dir requests pandas numpy

      - name: Test all API endpoints
        id: health
        run: |
          echo "üîç Testing deployed API..."
          python scripts/test_deployed_api.py

      - name: Health summary
        if: always()
        run: |
          if [ "${{ steps.health.outcome }}" == "success" ]; then
            echo "## ‚úÖ API Health: Good" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "All API endpoints are responding correctly." >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ùå API Health: Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "One or more API endpoints are not responding." >> $GITHUB_STEP_SUMMARY
          fi

  # ============================================
  # ENDPOINT AVAILABILITY (Every 6 hours)
  # ============================================
  endpoint-check:
    name: üåê Endpoint Availability
    runs-on: ubuntu-latest

    steps:
      - name: Check main endpoint
        run: |
          echo "üîç Checking main API endpoint..."
          response=$(curl -s -o /dev/null -w "%{http_code}" ${{ env.API_URL }}/)
          if [ "$response" == "200" ]; then
            echo "‚úÖ Main endpoint: OK (HTTP $response)"
          else
            echo "‚ùå Main endpoint: FAILED (HTTP $response)"
            exit 1
          fi

      - name: Check /metrics endpoint
        run: |
          echo "üîç Checking metrics endpoint..."
          response=$(curl -s -o /dev/null -w "%{http_code}" ${{ env.API_URL }}/metrics)
          if [ "$response" == "200" ]; then
            echo "‚úÖ Metrics endpoint: OK (HTTP $response)"
          else
            echo "‚ö†Ô∏è Metrics endpoint: FAILED (HTTP $response)"
          fi
        continue-on-error: true

      - name: Check /drift/status endpoint
        run: |
          echo "üîç Checking drift status endpoint..."
          response=$(curl -s ${{ env.API_URL }}/drift/status)
          if echo "$response" | jq . > /dev/null 2>&1; then
            echo "‚úÖ Drift status endpoint: OK"
            echo "$response" | jq .
          else
            echo "‚ùå Drift status endpoint: Invalid JSON"
            exit 1
          fi

      - name: Summary
        if: always()
        run: |
          echo "## üåê Endpoint Check Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Main API: ‚úÖ" >> $GITHUB_STEP_SUMMARY
          echo "- Metrics: ‚úÖ" >> $GITHUB_STEP_SUMMARY
          echo "- Drift Status: ‚úÖ" >> $GITHUB_STEP_SUMMARY

  # ============================================
  # PERFORMANCE BENCHMARK (Daily)
  # ============================================
  performance-test:
    name: ‚ö° Performance Benchmark
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 8 * * *' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --no-cache-dir requests numpy

      - name: Run performance benchmark
        run: |
          echo "‚ö° Running performance tests..."

          # Test prediction latency
          python3 << 'EOF'
          import requests
          import time
          import numpy as np

          API_URL = "${{ env.API_URL }}"

          # Create test sequence
          sequence = np.random.randn(20, 68).tolist()

          latencies = []
          successes = 0
          failures = 0

          print("Testing prediction latency (10 requests)...")
          for i in range(10):
              start = time.time()
              try:
                  response = requests.post(
                      f"{API_URL}/predict",
                      json={"sequence": sequence},
                      timeout=30
                  )
                  latency = time.time() - start
                  latencies.append(latency)

                  if response.status_code == 200:
                      successes += 1
                      print(f"  Request {i+1}: {latency:.3f}s ‚úÖ")
                  else:
                      failures += 1
                      print(f"  Request {i+1}: FAILED (HTTP {response.status_code})")
              except Exception as e:
                  failures += 1
                  print(f"  Request {i+1}: ERROR - {e}")

              time.sleep(0.5)

          if latencies:
              print(f"\nüìä Performance Summary:")
              print(f"  Successes: {successes}/10")
              print(f"  Failures: {failures}/10")
              print(f"  Avg Latency: {np.mean(latencies):.3f}s")
              print(f"  P50 Latency: {np.percentile(latencies, 50):.3f}s")
              print(f"  P95 Latency: {np.percentile(latencies, 95):.3f}s")
              print(f"  P99 Latency: {np.percentile(latencies, 99):.3f}s")

              # Alert if performance is poor
              if np.percentile(latencies, 95) > 2.0:
                  print("\n‚ö†Ô∏è WARNING: P95 latency > 2s")
                  exit(1)
          else:
              print("‚ùå All requests failed!")
              exit(1)
          EOF

      - name: Performance summary
        if: always()
        run: |
          echo "## ‚ö° Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Performance benchmark completed." >> $GITHUB_STEP_SUMMARY

  # ============================================
  # GENERATE TEST TRAFFIC (Daily)
  # ============================================
  generate-traffic:
    name: üìà Generate Test Traffic
    runs-on: ubuntu-latest
    if: github.event.schedule == '0 8 * * *' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install --no-cache-dir requests numpy

      - name: Send test predictions
        run: |
          echo "üìà Generating test traffic for monitoring..."

          python3 << 'EOF'
          import requests
          import numpy as np
          import time

          API_URL = "${{ env.API_URL }}"

          print("Sending 20 test predictions...")
          for i in range(20):
              sequence = np.random.randn(20, 68).tolist()

              try:
                  response = requests.post(
                      f"{API_URL}/predict",
                      json={"sequence": sequence},
                      timeout=30
                  )

                  if response.status_code == 200:
                      print(f"  Prediction {i+1}/20: ‚úÖ")
                  else:
                      print(f"  Prediction {i+1}/20: ‚ö†Ô∏è HTTP {response.status_code}")
              except Exception as e:
                  print(f"  Prediction {i+1}/20: ‚ùå {e}")

              time.sleep(0.5)

          print("\n‚úÖ Test traffic generation complete")
          EOF

  # ============================================
  # NOTIFICATION & REPORTING
  # ============================================
  notify-and-report:
    name: üìß Notifications
    runs-on: ubuntu-latest
    needs: [drift-detection, api-health, endpoint-check, performance-test]
    if: always()

    steps:
      - name: Generate summary report
        run: |
          echo "## üìä Monitoring Summary - $(date -u '+%Y-%m-%d %H:%M UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check job statuses
          if [ "${{ needs.drift-detection.result }}" == "failure" ]; then
            echo "- üî¥ **Drift Detection:** FAILED - Drift detected!" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.drift-detection.result }}" == "success" ]; then
            echo "- üü¢ **Drift Detection:** PASSED" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.drift-detection.result }}" == "skipped" ]; then
            echo "- ‚ö™ **Drift Detection:** Skipped (runs daily only)" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.api-health.result }}" == "success" ]; then
            echo "- üü¢ **API Health:** PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- üî¥ **API Health:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.endpoint-check.result }}" == "success" ]; then
            echo "- üü¢ **Endpoint Check:** PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "- üî¥ **Endpoint Check:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.performance-test.result }}" == "success" ]; then
            echo "- üü¢ **Performance:** PASSED" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.performance-test.result }}" == "failure" ]; then
            echo "- üî¥ **Performance:** FAILED - Latency too high" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.performance-test.result }}" == "skipped" ]; then
            echo "- ‚ö™ **Performance:** Skipped (runs daily only)" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**API URL:** ${{ env.API_URL }}" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow:** [View Details](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY

      - name: Send failure notification
        if: |
          needs.drift-detection.result == 'failure' ||
          needs.api-health.result == 'failure' ||
          needs.endpoint-check.result == 'failure' ||
          needs.performance-test.result == 'failure'
        run: |
          echo "=========================================="
          echo "‚ö†Ô∏è MONITORING ALERT - ACTION REQUIRED"
          echo "=========================================="
          echo "Time: $(date -u)"
          echo ""
          echo "One or more monitoring checks failed:"
          echo "  Drift Detection: ${{ needs.drift-detection.result }}"
          echo "  API Health: ${{ needs.api-health.result }}"
          echo "  Endpoint Check: ${{ needs.endpoint-check.result }}"
          echo "  Performance: ${{ needs.performance-test.result }}"
          echo ""
          echo "View logs: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          echo "=========================================="

      # Optional: Slack notification
      # Uncomment and add SLACK_WEBHOOK secret to enable
      # - name: Slack notification
      #   if: |
      #     needs.drift-detection.result == 'failure' ||
      #     needs.api-health.result == 'failure'
      #   uses: 8398a7/action-slack@v3
      #   with:
      #     status: failure
      #     text: |
      #       ‚ö†Ô∏è Production Monitoring Alert!
      #
      #       Drift: ${{ needs.drift-detection.result }}
      #       Health: ${{ needs.api-health.result }}
      #
      #       View: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
      #     webhook_url: ${{ secrets.SLACK_WEBHOOK }}
